{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P63ycwnX0H0"
   },
   "source": [
    "# Classification in Practice\n",
    "\n",
    "In this laboratory session we will gain some experience working with linear models for **classification** (we already saw how to do regression in a previous lab).\n",
    "\n",
    "We will follow the same general structure... with one exception -- for this laboratory we will begin with **synthetic** datasets. \n",
    "\n",
    "## Part 1: Working with Synthetic Classification Problems\n",
    "\n",
    "Generating synthetic datasets allows us to gain insight into how classifiers work. We will use some functionality of Scikit-learn to generate -- in a controlled manner -- synthetic classification problems of with various characteristics.\n",
    "\n",
    "**Note**: When working with these synthetic datasets we will *not* go to the trouble of generating train/test splits -- we are only interested in studying how classifiers work to **separate** the training data.\n",
    "\n",
    "### Exercise 1.1: Generating a dataset\n",
    "\n",
    "First, have a look at the documentation for [sklearn.datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs). This is one of the easiest ways to generate /simple/ classification problems. Study the documentation and then use `make_blobs` to generate an **EASY** dataset for a **two-class** classification problem with 100 samples per class and 2 input features. What does **EASY** mean? How can we determine, qualitatively, that the randomly generated dataset is \"easy\"? Make the problem *easy*, but not *too easy*.\n",
    "\n",
    "**Hint**: You will probably want to develop a *visualization* for datasets, and then *abstract* it into a function you can call later for other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: \"Solving\" the dataset\n",
    "\n",
    "Train a classifier that is *perfectly* classifies the dataset you created above. Any of the the three classifiers mentioned in the *Capsule Lecture* should do well. Try one, or try all three. Be sure to verify that the classifier does *indeed* classify all training points correctly.\n",
    "\n",
    "**Hint**: You might want to look at [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Visualizing the decision surfaces\n",
    "\n",
    "One of the best ways to understand how a classifier works is to visualize the decision boundaries. Use [sklearn.inspection.DecisionBoundaryDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) to create a visualization of the *dataset* and the *decision boundaries* for your classifier.\n",
    "\n",
    "**Note**: This is another great opportunity to apply *functional abstraction* and make a **reusable** visualization that you can reuse (for example in the next exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4: A harder dataset\n",
    "\n",
    "Repeat the exercises above, but first generate a **hard** dataset that is not linearly separable. Observe how linear classifiers fail to correctly classify the training data. How can we make these classifiers capable of \"solving\" this harder dataset? Try to find an explicit embedding that makes the problem linearly separable in the embedding space. Visualize the decision boundaries in the **original** space (you will need to spend some time with the documentation for `DecisionBoundaryDisplay` to make this work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: A Real Dataset\n",
    "\n",
    "In the second set of exercises we will work with a classic dataset for classification: the Iris Flower Classification Dataset. It is a fairly easy dataset to work with since it is low-dimensional and small. We start by loading the dataset, and then proceed with our usual protocol: \"playing\" with the data, creating train/test splits, and building and evaluating a first classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvUVJmT0X0H2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris classification dataset to get started.\n",
    "ds = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Play with the data\n",
    "\n",
    "Use everything we have learned about *exploratory data analysis* to study the nature and characteristics of this classification problem. Are the classes equally represented? How many features are there in input? How are input features scaled? **Be creative** and **summarize** your findings with analysis along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data playground here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FXCheZBX0H7"
   },
   "source": [
    "### Exercise 2.2: Your Turn\n",
    "\n",
    "Design an experiment to decide which of the three classifiers we have seen performs best on this dataset. Some things to keep in mind:\n",
    "+ You will probably want to use [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to perform K-fold cross-validation to ensure you are *robustly* estimating performance.\n",
    "+ All three of the classifier models we have used support **regularization**, which might be an interesting hyperparameter to cross-validate. Unfortunately they use slightly different terminologies:\n",
    "  + in `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` it is called `shrinkage`\n",
    "  + in `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` it is called `reg_param`\n",
    "  + and in `sklearn.svm.LinearSVC` it is called `C` -- but the regularization performed is 1/C!\n",
    "  \n",
    "**Important**: Remember to *document* your findings and analyses along the way. Summarize and justify your final conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAv5h5pcX0IP"
   },
   "source": [
    "---\n",
    "## Part 3: A Harder Dataset\n",
    "\n",
    "OK, now let's switch to the **digits** dataset which should be a bit more challenging. This should be easy by now, and if you have been careful about **functional abstraction** you should be able to reuse much of the functionality from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "KBElNvo_X0IQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "ds_digits = load_digits()\n",
    "df_digits = pd.DataFrame(ds_digits.data)     # Why are there no column names?\n",
    "targets_digits = pd.Series(ds_digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Exploratory data analysis\n",
    "\n",
    "You know the drill, see what this dataset is made of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data playground here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3xOHO3-7UdJ"
   },
   "source": [
    "### Exercise 3.2: Visualize Some Images\n",
    "\n",
    "Now we are working with (very small) images. There a useful Matplotlib function for visualizing images is `imshow()`. Use it like this:\n",
    "\n",
    " `plt.imshow(df_digits.iloc[0,:].to_numpy().reshape(8,8), cmap='gray')`\n",
    "\n",
    " **NOTE**: The Pandas DataFrame structure is kind of getting in our way here -- we have to extract a row, then **convert** it to a numpy array, and then **resize** it to (8,8) to view it.\n",
    "\n",
    "View some images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "kYWRx1ExFe5m"
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Find the best classifier (of the ones we have seen)\n",
    "\n",
    "You should now be very familiar with this game and if you consolidated the pieces you used before, this exercise should be easy. As always, document, summarize, and justify your conclusions and analyses. If you like, try out some other classifiers to see if you can get better (but still robust!) performance on the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
