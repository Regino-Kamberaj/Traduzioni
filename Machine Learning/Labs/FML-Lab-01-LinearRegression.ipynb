{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "354iyEmhWYRc"
   },
   "source": [
    "# CODE OF CONDUCT V1.0\n",
    "\n",
    "This Code of Conduct outlines principles for the ethical and transparent use of Large Language Models (LLMs) and existing and internet resources, ensuring integrity and accountability in your laboratory submissions. This first version of the FML Laboratory Code of Conduct was developed in a brainstorming session with **ChatGPT Version 2**, modifying its proposal to the specifics of the FML Laboratories and concentrating on the transparent collaboration with classmates and transparent use of LLMs.\n",
    "\n",
    "#### **1. Transparency in LLM Use**\n",
    "- **Clear Disclosure:** Explicitly state when Large Language Models (LLMs) are used in any part of the process, including data analysis, code generation, or writing.\n",
    "- **Model Limitations:** Acknowledge the inherent limitations of LLMs, such as potential biases, and make clear where human intervention was applied to verify results or to modify/augment produced code.\n",
    "\n",
    "#### **2. Proper Attribution and Documentation**\n",
    "- **Attribution:** Provide appropriate citations and credits for all external resources, including code, data, and models.\n",
    "- **Clear Documentation:** Maintain detailed records of tools, methods, and models used, ensuring transparency and reproducibility in your submitted laboratory solutions.\n",
    "\n",
    "#### **3. Collaboration and Individual Work**\n",
    "- **Sharing Solutions:** While collaboration and discussing ideas with classmates are encouraged, solutions to assignments or projects should be your own. Do not copy or submit work created by others, including code or models, as your own.\n",
    "- **Submission Integrity:** All submitted work must reflect your own understanding and effort. If external tools, LLMs, online resources, or code from your classmates were used, they must be properly documented, but the final submission must be an individual effort.\n",
    "\n",
    "#### **4. Accountability**\n",
    "Non-compliance with these guidelines will be subject to review by the course exam commission, with possible disciplinary actions.\n",
    "\n",
    "---\n",
    "# Linear Regression in Practice\n",
    "\n",
    "In this lab we will work through an extended example of exploratory data analysis and supervised machine learning using the California Housing Price Dataset. This dataset consists of data about housing characteristics and prices in many districts of the state of California. The **task** this dataset asks us to solve is estimating the median house value in a district from a set of independent housing characteristics.\n",
    "\n",
    "**Note**: the exercises are inline in this notebook and *not* at the end. The exercises will ask you to write some code and sometimes to provide some analysis of your findings in Markdown cells at the end of the exercise.\n",
    "\n",
    "**Also Note**: This first laboratory is broken down into many small exercises to guide you through the development of good methodological practice. The subsequent laboratories will be *much less guided*.\n",
    "\n",
    "The main objectives of this laboratory are:\n",
    "+ For you to gain familiarity with using Python for numerical programming in Jupyter Notebooks, Google Colaboratory, or *[INSERT TOOL OF CHOICE]*.\n",
    "+ For you to gain familiarity with working with data in Numpy, Pandas, and Scikit-learn.\n",
    "+ For you to learn how to use *visualization* as a tool for understanding the nature of machine learning problems and gain insight into potential solutions.\n",
    "+ To learn to use Scikit-learn to solve simple univariate regression problems and validate your solutions.\n",
    "+ To learn the value of *encapsulation* and *abstraction* of pipeline code for making experiments *reproducible*.\n",
    "\n",
    "\n",
    "# Part 1: Warming Up\n",
    "\n",
    "In this first set of exercises we will analyze our dataset and build a simple linear regression pipeline. This is a fairly typical task that is asked of anyone working with Data Science: Here is some data, do something useful with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuH_jItJW0x3"
   },
   "source": [
    "## Step 1: Data Modeling\n",
    "\n",
    "OK, let's get started. The first thing we want to do is get our dataset loaded and start to get a feel for it. This is always a good idea -- we *play* with the data first in order to get a better understanding of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "JooRxF-EWy_K"
   },
   "outputs": [],
   "source": [
    "# Initial imports -- these are fairly standard.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import the function that will download the dataset.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the sklearn version of the California Housing dataset.\n",
    "ds = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WafFRz0hXbaj"
   },
   "source": [
    "### Exercise 1.1: Poking Around\n",
    "\n",
    "Spend some time looking at the elements of the `ds` we just loaded (it's a python `dict`). Find the description of the dataset and make sure you understand what the features are and what the targets variable is. **Hints**: to get the keys of the dictionary, use: `ds.keys()`.\n",
    "\n",
    "We are going to construct a Pandas `DataFrame` in the next exercise. Where can you get reasonable column names from the sklearn dataset object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis**: Modify this Markdown cell with the results of your playing around in the above code cell. The point of Notebooks is to build self-documenting, executable, and reproducible analyses of your work -- so **document** on the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHk-0p74YvlK"
   },
   "source": [
    "### Exercise 1.2: Creating a Pandas DataFrame\n",
    "\n",
    "OK, now we can create the `DataFrame` to hold our independent variables and a `Series` to hold the target values. Make sure you use good column names when constructing the `DataFrame`. Some relevant documentation: [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and [pandas.Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "7MMFXfX7YAFA"
   },
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame for our dataset and a Pandas Series for the targets.\n",
    "\n",
    "# Your code to build the DataFrame here (replace None)\n",
    "df = None\n",
    "\n",
    "# Your code to build the target Series here (replace None)\n",
    "targets = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W45Rdpeuago-"
   },
   "source": [
    "### Exercise 1.3: Examining the Data\n",
    "\n",
    "Study the *descriptive statistics* of the data. Do you notice anything \"strange\" about any of the features? Are the features scaled similarly? **Hint**: Use the `.describe()` method on the DataFrame you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis Here** (in Markdown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftU3jmhwgw2I"
   },
   "source": [
    "---\n",
    "## Step 2: Visualization\n",
    "\n",
    "OK, now that we have a bit of a *feel* for our data, let's get a better idea about it through visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5WZhoeOjeRd"
   },
   "source": [
    "### Exercise 2.1: Visualizing the Target\n",
    "Create a plot to study the **distribution** of our target values. The best tool for that is a **histogram**. Search for this functionality in the Matplotlib documentation.\n",
    "\n",
    "**Note**: In addition to *histograms*, try out the Seaborn function `distplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your visualization code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis Here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_9GRJSQkhAg"
   },
   "source": [
    "### Exercise 2.2: Subplots\n",
    "Now create a multi-plot figure to visualize the distributions of **all** of the independent features in the dataset. Make sure you use `figsize=` to resize the figure appropriately.\n",
    "\n",
    "A few things that will help with this:\n",
    "+ If you want to index columns by **integer** indices, use the `.iloc()` method (e.g. `df.iloc[:,1]`).\n",
    "+ If you extract a column as a `Series` from a `DataFrame`, you can recover its name with the `name` attribute.\n",
    "+ Encapsulate you plotting code in a **function** you can call later.\n",
    "\n",
    "**Super Hint**: Pandas already has this functionality **built-in**. If you can find it, use it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "0stkGgwJkfcS"
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis Here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfWcikC8nEbd"
   },
   "source": [
    "## Step 3: Split you Data\n",
    "\n",
    "A very important step. Now we will split our `DataFrame` into training and testing splits.\n",
    "\n",
    "### Exercise 3.1: Create a Split\n",
    "Now we need to create our training and testing splits. Read the documentation for `sklearn.model_selection.train_test_split()`. Use this function to create a **training** split with 75% of the data, and a **test** split with 25% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "LYXhgQhHlDBx"
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into 75-25 train/test split -- replace the [None]*4 with your code.\n",
    "(Xtr, Xte, ytr, yte) = [None]*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: *My* convention for data and label matrices is `Xtr, ytr` (training) and `Xte, yte` (testing) because the `X` and `y` helps link the *code* to the *math*. Start developing your own style to help you organize (and eventually share) your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd_imO7rB-ey"
   },
   "source": [
    "### Exercise 3.2: Fit a LinearRegression\n",
    "Finally some machine learning. Study the documentation for `class sklearn.linear_model.LinearRegression`. Then write some code to fit a linear regression model to your **training** split. Try out your model by computing predictions on some data (use the `model.predict()` method).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "LxFCBMaqAPQh"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Your code here.\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f4EuH5LCzM2"
   },
   "source": [
    "### Exercise 3.3: Evaluate your Model\n",
    "Write some code to compute the root mean-squared error (RMSE) and mean absolute error (MAS) for you model predictions. Try it on both the **test** and **training** splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ISex3vviCNAS"
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis Here**: Why is the performance on the train set different than that on the test split? What if you change the proportion of training to test data in your splits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gTBZCJ7DXBm"
   },
   "source": [
    "### Exercise 3.4: Visualizing the Results\n",
    "Now I want you to write a function that makes a **residual plot** of the data and the model predictions. This plot should show, for each data point, the **signed error** (i.e. y - predicted) of the model prediction. Do you notice any **patterns** in the errors? Can you link this to previous analyses you made? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "-tlKd0moCllW"
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkyv8K8BGqDf"
   },
   "source": [
    "## Step 4: Repeat.\n",
    "\n",
    "Now you should put all of the pieces together into a repeatable, reproducible pipeline.\n",
    "\n",
    "### Exercise 4: The Pipeline\n",
    "Write a function (or even just code in the cell that calls previously defined functions) that runs an **experiment**:\n",
    "1. Splitting data\n",
    "1. Instantiating the model\n",
    "1. Fitting the model\n",
    "1. Evaluating the model\n",
    "1. (Maybe) Visualizing results\n",
    "\n",
    "Experiment with different splits to see if the results are the same. Try using more or less training data with respect to test data. Observe how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "QTY5v32hFs8-"
   },
   "outputs": [],
   "source": [
    "# Your pipeline code here.\n",
    "def pipeline(model, df, targets, train_size=0.75):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis Here**: Experiment with different splits to see if the results are the same. Try using more or less training data with respect to test data. Observe how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Part 2: Improving our Regressor\n",
    "\n",
    "Now that we have a simple, baseline linear regression result, let's see if we can't improve on it. This is where the real work begins, and where it is **super** important to ensure that the conclusions we draw are *valid*.\n",
    "\n",
    "**Questions**: Are our independent variables *scaled* similarly? Does our model have *high variance* -- that is, if we fit it to a new training sample, does the result vary dramatically?\n",
    "\n",
    "### Exercise 5: Increasing Model Capacity\n",
    "\n",
    "Check out the documentation for `sklearn.preprocessing.PolynomialFeatures`. Map the independent variables onto a **polynomial** basis of variable order. Fit your model using your pipeline from above and observe its behavior for different degree polynomial embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Hyperparameter Selection and Cross-validation\n",
    "\n",
    "How should we select the correct *degree* for our polynomial basis? Is the performance on the *training* set equal to the performance on the *test* set? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Final Analysis Here**: Summarize the conclusions you can make about the best hyperparameter settings for this dataset. How do you know your conclusions are supported by the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 3: Optional\n",
    "\n",
    "### Exercise 7 (BONUS): Ordinary Least Squares with Gradient Descent\n",
    "This is an *optional* exercise, but I think everyone should implement gradient descent at least once in their life, so if you like a challenge this assignment can earn you up to five (5) bonus points out of 100 on this assignment.\n",
    "\n",
    "#### Exercise 7.1: The Loss Function\n",
    "Recall that the loss function for parameters $\\mathbf{w}$ on dataset $\\mathcal{D} = \\{ (\\mathbf{x}_n, y_n)\\}_{n=1}^{N}$ we use for least squares regression is (assume that the inputs $\\mathbf{x}$ have already been augmented with $x_0 = 1$ to account for the bias parameter $w_0$):\n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{D}, \\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} (\\mathbf{w}^T \\mathbf{x}_n - y_n)^2\n",
    "$$\n",
    "\n",
    "Start by implementing a function `error(w, Xs, ys)` that computes this loss function. This function should be *independent* of feature dimensionality and **do not use loops**!\n",
    "\n",
    "Test this function using data from the California Housing dataset (after augmenting it with a constant 1 in the first dimension!) with the parmaters $\\mathbf{w}$ set to the solution found by `LinearRegression`. Compare it to randomly initialized $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(w, Xs, ys):\n",
    "    pass # Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7.2: The Gradient of the Loss Function\n",
    "Now we need the *gradient* of the loss function in order to improve our solution. Recall that the gradient of the loss is:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathcal{D}, \\mathbf{w}) &=& \\nabla_{\\mathbf{w}} \\frac{1}{2} \\sum_{n=1}^{N} (\\mathbf{w}^T \\mathbf{x}_n - y_n)^2 \\\\\n",
    "&=& \\frac{1}{2} \\sum_{n=1}^{N} \\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_n - y_n)^2 \\mbox{ (by linearity of the gradient)} \\\\\n",
    "&=& \\sum_{n=1}^{N} (\\mathbf{w}^T \\mathbf{x}_n - y_n) \\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_n) \\mbox{ (by gradient rule for quadratics and chain rule)}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Write a Python function `grad_error(w, Xs, ys)` to compute this gradient of the loss function for a given $\\mathbf{w}$ and dataset $\\mathcal{D}$. Again, if you find yourself writing loops, your probably doing something wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_error(w, Xs, ys):\n",
    "    pass # Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7.3: Gradient Descent\n",
    "Now we have everything we need to, starting from a randomly initialized $\\mathbf{w}$ and some data $\\mathcal{D}$, iteratively improve our solution (up to some numerical tolerance). Recall that our gradient descent update rule is:\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathcal{D}, \\mathbf{w}_{t})\n",
    "$$\n",
    "Write a Python function `gradient_descent(w, Xs, ys, eta, tol=1e-5)` to perform gradient descent to convergence (i.e. until the total loss between iterations is less than `tol`). Here you can use a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, Xs, ys, eta, tol=1e-5):\n",
    "    pass # Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your implementation of gradient descent to solve the regression problem for the California Housing dataset. You will probably have to play with different learning rates, but verify that you end up with a solution (very) close to the one found by Scikit-learn's `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Numerical Programming and Reproducible Science.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
