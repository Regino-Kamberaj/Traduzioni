{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfebf3-12ca-40cb-b2c6-6af00f86b975",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A High-entropy Introduction to Pytorch\n",
    "\n",
    "**Important**: You **must** install Pytorch in your Anaconda environment for this laboratory. The easiest way to do this is to just install the CPU version of Pytorch like this:\n",
    "\n",
    "```\n",
    "conda activate FML\n",
    "conda install -c conda-forge pytorch torchvision\n",
    "```\n",
    "\n",
    "**Note**: If you have an Nvidia GPU on your computer you can also install the GPU-enabled version of Pytorch which will **greatly** improve performance for more complex models and larger datasets. However, it can be very hard to get all of the versions of the required libraries to match correctly... During the laboratory we can look at it together if you are interested.\n",
    "\n",
    "After installing Pytorch, use the next cell to verify that the installation is working. If it prints a 3x3 sensor, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace0060-1c75-4705-82c8-58bd36b6ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will still need numpy for some things.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is the main torch namespace.\n",
    "import torch\n",
    "\n",
    "# If this works, things should be OK.\n",
    "print(torch.randn((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c44385-ed08-4256-92f6-26f9813491e7",
   "metadata": {},
   "source": [
    "In this notebook and accompanying video I will barely scratch the surface of what Pytorch is and what it can do. Please see the excellent [Introduction to Pytorch](https://pytorch.org/tutorials/beginner/introyt.html) series on YouTube for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d442347-033e-48b7-8d14-262901ad3497",
   "metadata": {},
   "source": [
    "## Simple Gradient Descent in Pytorch\n",
    "\n",
    "All computations done with pytorch \"live\" in a computational graph. This is transparent to us, until... it *isn't*. Let's just say it takes some getting used to.\n",
    "\n",
    "We will reproduce an early example of gradient descent using pytorch. We will find the minimum of a translated parabola. First let's see how to build the computational graph for a parabolic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8badd9a-2ce0-4f92-bcff-219969703158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the definition of our translated parabola. Note that this is *identical*\n",
    "# to how we would define this in numpy.\n",
    "def tpara(x, t=0):\n",
    "    return (x+t)**2\n",
    "\n",
    "# Plot the parabola over a range of values.\n",
    "rng = np.arange(-10, +10, 0.01)\n",
    "plt.plot(rng, [tpara(x, 2.5) for x in rng])\n",
    "_ = [plt.xlabel('x'), plt.ylabel('f(x)')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e91184-a610-4fb5-892f-44d704bd5e05",
   "metadata": {},
   "source": [
    "OK, but what about **pytorch** in all of this?! Well, if we call the function with **pytorch tensors** instead of constant values, we will construct a **computational graph** of the parabola function instead of actually performing the computation.\n",
    "\n",
    "Let's see how we can build graphs and perform calculations with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570de35e-3755-45ca-86b3-0167587b20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an input (and prepare it for computing gradient).\n",
    "x = torch.randn((1,), requires_grad=True)\n",
    "\n",
    "# Build the graph representing the computation of the parabola.\n",
    "foo = tpara(x, 2.5)\n",
    "\n",
    "# Inspect the output.\n",
    "print(f' Input  (a tensor): {x}')\n",
    "print(f' Output (a tensor): {foo}')\n",
    "print(f'Duplicating output: {(x.item() + 2.5)**2}')  # The item() method returns the value of a 0-degree (scalar) tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660358d-fdfa-4616-94ce-753bf5e6639e",
   "metadata": {},
   "source": [
    "Now let's plot the parabola using the pytorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e404-39d4-4781-8148-5eb696b7cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the use of item() here and how we *must* pass a tensor in input.\n",
    "plt.plot(rng, [tpara(torch.tensor(x, requires_grad=True).item(), 2.5) for x in rng])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f990fe-b76c-448c-a0e2-94cbae8b483c",
   "metadata": {},
   "source": [
    "## Finding the minimum.\n",
    "\n",
    "OK, so what? This just seems like the same functionality of the numpy version but with extra steps. Well, the whole reason to use pytorch (or any graph-based numerical programming framework) is the facilitate the computation of **gradients**. Let's compute the derivative of our parabola with respect to $x$. In particular, as we all know:\n",
    "\n",
    "$$ \\frac{d}{dx} (x+c)^{2} = 2(x+c). $$\n",
    "\n",
    "Let's compute this numerically for specific values of $x$ and $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a4619-a9e1-4e70-b9c4-aaf7d9d6c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a specific x and c.\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "t = 1.34\n",
    "\n",
    "# Forward pass, then backward pass to compute gradient.\n",
    "foo = tpara(x, t)\n",
    "foo.backward()\n",
    "\n",
    "# Is it correct?\n",
    "print(f'Computed gradient: {x.grad}')\n",
    "print(f'    True gradient: {2*(x.item() + t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244f664-f6a8-48db-9017-e7055d1532b7",
   "metadata": {},
   "source": [
    "OK, now we can implement gradient descent to find the minimum of our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545658a-886c-47e0-825c-a517f09b2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at some random point, fix a translation t.\n",
    "x = torch.randn((), requires_grad=True, dtype=torch.float32)\n",
    "t = 7.5\n",
    "\n",
    "# Iterate gradient steps for a fixed number of updates.\n",
    "points = []\n",
    "for i in range(20):\n",
    "    # Need to *zero* the gradient before every iteration!\n",
    "    if x.grad:\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    # The forward and backward passes.\n",
    "    current = tpara(x, t)\n",
    "    current.backward()\n",
    "    \n",
    "    # Save the current value for plotting.\n",
    "    points.append((x.item(), current.item()))\n",
    "    \n",
    "    # And make the gradient step -- we need the no_grad() otherwise\n",
    "    # we *add* this operation to the graph!\n",
    "    with torch.no_grad():\n",
    "        x -= 0.9 * x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbc019-3e2a-4958-86e2-a9debde9773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results.\n",
    "rng = np.arange(-7.6, 1.0, 0.01) \n",
    "plt.plot(rng, [tpara(torch.tensor(x, requires_grad=True).item(), t) for x in rng])\n",
    "plt.scatter([x for (x, _) in points], [y for (_, y) in points], c='red')\n",
    "plt.plot([x for (x, _) in points], [y for (_, y) in points])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "_ = plt.title(f'Final minimum: x={points[-1][0]} y={points[-1][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cfc6d-cb11-45ca-8bff-df530b83d472",
   "metadata": {},
   "source": [
    "## Deep Linear Regression in Pytorch\n",
    "\n",
    "OK, enough dinking around... Let's build a deep network for solving a linear regression problem. When working with Deep Learning we usually we don't build the network tensor by tensor. Instead we will use the tools provided by pytorch specifically for working with deep models.\n",
    "\n",
    "First, let's setup the regression problem just like we did way back in the first lectures of the course a sinusoidal function that we sample from **with noise**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00623f83-8452-4e09-8b49-54833b5c11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for numerical programming.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The true function -- that is, what we want to estimate from data.\n",
    "def true_fn(x):\n",
    "    return 100 * x*np.sin(2*np.pi*x)\n",
    "\n",
    "# The observation process: true + Gaussian Noise.\n",
    "def observed_fn(x, beta):\n",
    "    noise = np.random.normal(0, 1, size=x.shape) * beta(x)\n",
    "    return true_fn(x) + noise\n",
    "\n",
    "# How we actually observe a function: uniformly generate N random\n",
    "# xs in rng, apply the passed function to the xs.\n",
    "def sample_from_fn(fn, N, rng=[0.0, 1.0]):\n",
    "    xs = np.sort(np.random.random(size=(N,)) * (rng[1]-rng[0]) + rng[0])\n",
    "    return (xs, fn(xs))\n",
    "\n",
    "# Sample 300 points from the true function and plot them (in green).\n",
    "(true_xs, true_fxs) = sample_from_fn(true_fn, 300)\n",
    "plt.plot(true_xs, true_fxs, 'g')\n",
    "\n",
    "# Now sample 20 points from the observed function and plot\n",
    "# the samples (in blue).\n",
    "(observed_xs, observed_fxs) = sample_from_fn(lambda x: observed_fn(x, beta=lambda x: 1/0.2), 20)\n",
    "_ = plt.scatter(observed_xs, observed_fxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99544622-5f4c-40db-a874-c2cb53946ef6",
   "metadata": {},
   "source": [
    "Now we will define our model. However, since we generated all of our data with numpy we **must** first convert it to pytorch tensors before we can work with it. This is one of the main stubling blocks with pytorch -- converting data back and forth between pytorch and numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d4de6-3b0c-4e1f-b134-f2181e2c1383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pytorch imports.\n",
    "import torch\n",
    "\n",
    "# Convert our dataset to pytorch tensors.\n",
    "t_observed_xs  = torch.tensor(observed_xs.reshape(len(observed_xs), 1), dtype=torch.float32)\n",
    "t_observed_fxs = torch.tensor(observed_fxs.reshape(len(observed_fxs), 1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26162583-47a1-4706-be79-fd7fbfeadc03",
   "metadata": {},
   "source": [
    "As mentioned above, we will make extensive use of the `torch.nn` namespace. It contains a **ton** of functionality for defining layers in Deep Neural Networks. In particular we are using:\n",
    "\n",
    "+ `torch.nn.Sequential`: Which defines a model that consists of sequential application of one or more computational layers.\n",
    "+ `torch.nn.Linear`: Our good old linear layer, in this case one that takes an input of size 1 and produces an output of size 1. This is just our old friend Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d7f6c-01ba-4112-986b-927b47295796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model, starting with simple linear regression (no hidden layers).\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bba53-fc06-41b0-8f8a-2999cfc0e2b1",
   "metadata": {},
   "source": [
    "Now we can train the model with gradient descent. Note that we are using **batch gradient descent** here -- we compute the loss over the *entire* dataset to then compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d38e80-23b6-4188-a9af-1d141452710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters.\n",
    "epochs = 1000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# The gradient descent training loop.\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    # Erase previous gradients.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass (plus loss).\n",
    "    output = model(t_observed_xs)\n",
    "    loss = torch.nn.functional.mse_loss(output, t_observed_fxs)\n",
    "    \n",
    "    # Backpropagate gradients.\n",
    "    loss.backward()\n",
    "    \n",
    "    # And perform gradient descent.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "    # Save the current loss for later.\n",
    "    losses.append(loss.detach().numpy())\n",
    "            \n",
    "# And plot the losses.\n",
    "_ = plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c7acf-91b9-4239-ae97-01f9fb157c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot our polynomial estimate.\n",
    "predictions = model(torch.tensor(true_xs.reshape(len(true_xs), 1), dtype=torch.float32)).detach().numpy()\n",
    "plt.plot(true_xs, predictions, 'r')\n",
    "plt.scatter(observed_xs, observed_fxs)\n",
    "plt.plot(true_xs, true_fxs, 'g')\n",
    "_ = plt.ylim([-80, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26972d9-40a7-4a53-b635-4825742067c9",
   "metadata": {},
   "source": [
    "## Going Deeper\n",
    "\n",
    "OK, well that worked about as well as we knew it would. Let's train a deep model with hidden layer and see if we can do better. I will take this opportunity to encapsulate *some* of the training functionality and to produce an animation of the training. The deep network we will use two hidden layers of 100 units.\n",
    "\n",
    "**Note**: This cell takes a long time to compute, and the animation functionality of Matplotlib is *very* primitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94351f1f-b764-4c5e-9355-2ad3544d667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib\n",
    "from IPython.display import HTML\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "\n",
    "# Define a fresh model.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 100),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "# Setup the basic plot.\n",
    "(fig, ax) = plt.subplots()\n",
    "predictions = model(torch.tensor(true_xs.reshape(len(true_xs), 1), dtype=torch.float32)).detach().numpy()\n",
    "regressor, = ax.plot(true_xs, predictions, 'r')\n",
    "ax.plot(true_xs, true_fxs, 'g')\n",
    "ax.scatter(observed_xs, observed_fxs)\n",
    "ax.set_ylim([-85, 35])\n",
    "\n",
    "# This should be in a class to encapsulate everything better.\n",
    "losses = []\n",
    "def update():\n",
    "    # Erase previous gradients.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass (plus loss).\n",
    "    output = model(t_observed_xs)\n",
    "    loss = torch.nn.functional.mse_loss(output, t_observed_fxs)\n",
    "    \n",
    "    # Backpropagate gradients.\n",
    "    loss.backward()\n",
    "    \n",
    "    # And perform gradient descent.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    return model(torch.tensor(true_xs.reshape(len(true_xs), 1), dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "def iterate(i, learning_rate, model):\n",
    "    regressor.set_data(true_xs, update())\n",
    "    return [regressor]\n",
    "\n",
    "# Training hyperparameters.\n",
    "epochs = 5000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# And animate!\n",
    "anim = animation.FuncAnimation(fig, lambda i: iterate(i, learning_rate, model), frames=epochs, interval=10, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16942d14-e878-4e59-b989-bb93160dbeb3",
   "metadata": {},
   "source": [
    "## Caveats and Disclaimers\n",
    "\n",
    "This notebook provides a **minimal** introduction to the important concepts and ways of using Pytorch. That said, we **NEVER** implement our own gradient descent training looks as shown in these examples. Well, almost never... We will be using the optimizers provided in the [torch.optim](https://pytorch.org/docs/stable/optim.html) package which wraps Stochastic Gradient Descent with all of the bells and whistles to train real models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
