
---
# **00 â€“ Informazioni Amministrative**  

MSc in Intelligenza Artificiale  
MSc in Ingegneria Informatica  
**Marco Lippi**  
ğŸ“§ [marco.lippi@unifi.it](mailto:marco.lippi@unifi.it)

---
**Indice dei contenuti**

## â–¶ Organizzazione del corso

---
**Contenuto del corso (1/5)**

Introduzione e concetti generali
- Intelligenza artificiale sicura e affidabile: motivazioni e sfide
- Intelligenza artificiale spiegabile: tassonomia, metriche
- InterpretabilitÃ  progettuale
- SpiegabilitÃ  dei modelli black-box
- Applicazioni: giustizia, medicina, **sistemi critici per la sicurezza**

---
**Contenuto del corso (2/5)**

Modelli interpretabili
- Approcci iniziali: alberi decisionali, modelli lineari generalizzati, liste di regole
- Explainable Boosting Machines
- Modelli basati su prototipi, incorporamento di concetti
- Alberi decisionali ottimali
- Programmazione logica induttiva
- Argomentazione

---
**Contenuto del corso (3/5)**

Spiegazioni post-hoc
- SHAP
- LIME
- Attenzione, mappe di salienza
- Analisi di sensibilitÃ 

---
**Contenuto del corso (4/5)**

Intelligenza artificiale **neuro-simbolica**
- Combinazione di IA simbolica e sottosimbolica
- Logica di Markov, DeepProbLog
- Modelli a colli di bottiglia concettuali

---
**Contenuto del corso (5/5)**

Applicazioni e sessioni pratiche
- Librerie Python per metodi specifici
- Manipolazione e visualizzazione dei dati
- Ispezione dei modelli
- Presentazione dei risultati, calcolo delle metriche
- Problemi reali e casi studio

---
**Orario delle lezioni**

ğŸ“Œ **Lezioni:**
- MartedÃ¬, 14:00-16:00, aula S12 @ Santa Marta (14:00-15:30)
- GiovedÃ¬, 16:00-18:00, aula 177 @ Santa Marta (16:00-17:30)

ğŸ“Œ **Orario di ricevimento:**
- MartedÃ¬, 11:30-13:30 (ufficio @ DINFO, Santa Marta)
- Oppure su appuntamento via email
- In ogni caso, scrivetemi sempre in anticipo!

---
**Materiale didattico**

Il materiale didattico sarÃ  disponibile su **Moodle**:
- Slide (a volte contenenti solo riassunti degli argomenti)
- Appunti dalle lavagne
- Riferimenti a articoli scientifici e capitoli di libri

ğŸ“¢ **Iscrivetevi alla pagina Moodle per:**
- Informazioni sul corso
- Variazioni di orario, annunci
- Aggiornamenti del materiale didattico
- Informazioni sullâ€™organizzazione dellâ€™esame

---
**Esame**

Lâ€™esame consiste in una prova orale sui contenuti del corso:
- **Iscrizione** tramite SOL
- **Date disponibili:**
    - Estate: 17 giugno, 1Â° luglio, 21 luglio
    - Autunno: 1Â° settembre
    - Inverno/primavera: date da definire

---
**Progetti di laboratorio**

Gli studenti che hanno scelto il **Laboratorio (3 CFU)** nel loro piano di studi svolgeranno un progetto sullâ€™applicazione delle tecniche di **XAI** a problemi reali:

- Diritto
- Medicina
- Sistemi critici per la sicurezza
- Astronomia
- Bibliometria
- â€¦

---

# **01 â€“ Introduzione**  

---
**Indice dei contenuti**

â–¶ Concetti generali  
â–¶ Bias e correttezza  
â–¶ Valutazione  
â–¶ Analisi esplorativa dei dati  
â–¶ Esempi

---

## â–¶ Concetti generali  

---
**Crediti**

Queste slide sono in gran parte un adattamento di materiale esistente, tra cui:

- _Explainable Artificial Intelligence_ (F. Lecue et al., 2023)
- _Explainable and Trustworthy Artificial Intelligence_ (Politecnico di Torino, 2023)
- _Explainable AI_ (Harvard University, 2023)

ğŸ”— **Riferimenti:**

- [XAI Tutorial 2023](https://xaitutorial2023.github.io/)
- [Politecnico di Torino](https://dbdmg.polito.it/dbdmg_web/2024/explainable-and-trustworthy-ai-2023-2024/)
- [Interpretable ML Class](https://interpretable-ml-class.github.io/)

---
**Cos'Ã¨ XAI?**

![[Pasted image 20250311082740.png]]

L'Intelligenza Artificiale Spiegabile (_Explainable AI_, XAI) studia e sviluppa **metodi** per rendere **accessibile** e **interpretabile** la logica interna e i risultati dei modelli di intelligenza artificiale, rendendoli **comprensibili agli esseri umani**.

---
**Modelli "scatola nera"**

![[Pasted image 20250311082850.png]]

---
**Modelli "scatola nera"**

![[Pasted image 20250311082923.png]]

---
**PerchÃ© abbiamo bisogno della XAI?**

1ï¸âƒ£ **Per comprendere le decisioni dell'IA**  

![[Pasted image 20250311083033.png]]

---
**PerchÃ© abbiamo bisogno della XAI?**

2ï¸âƒ£ **Per migliorare i modelli di IA**  

![[Pasted image 20250311083124.png]]

---
**PerchÃ© abbiamo bisogno della XAI?**

3ï¸âƒ£ **Per costruire sistemi di IA affidabili e sicuri**

![[Pasted image 20250311083138.png]]

---
**Il problema dell'allineamento**

ğŸ“– _"Se utilizziamo un meccanismo che non possiamo controllare in modo efficaceâ€¦ dobbiamo essere certi che lo scopo inserito nella macchina sia quello che desideriamo davvero."_                                         â€” Norbert Wiener, 1960

---
**Il problema dell'allineamento**

ğŸ¤– _Le Tre Leggi della Robotica_ (Isaac Asimov, 1942):  
	1.  Un robot non puÃ² arrecare danno a un essere umano, nÃ© permettere che un essere umano subisca danni per la sua inazione.  
	2. Un robot deve obbedire agli ordini impartiti dagli esseri umani, a meno che ciÃ² non contrasti con la Prima Legge. 
	3. Un robot deve proteggere la propria esistenza, purchÃ© questa protezione non contrasti con la Prima o la Seconda Legge.

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

![[Pasted image 20250311083519.png]]

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

![[Pasted image 20250311083536.png]]

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

![[Pasted image 20250311083915.png]]

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

**Esempio: Predizione del rischio di polmonite con IA**
Un modello di IA potrebbe imparare erroneamente che i pazienti asmatici hanno un rischio minore di complicazioni perchÃ© sono ricoverati **direttamente in terapia intensiva**. Questo porta a conclusioni fuorvianti se si considerano solo i dati storici!

---
**Modelli black-box possono essere presi in giro**

![[Pasted image 20250311084154.png]]

---
**Bias nei dati**

ğŸ“Œ "PerchÃ© dovrei fidarmi del tuo modello?" (Ribeiro et al.)  => Motivare ogni predizione di ogni classificatore

![[Pasted image 20250311084216.png]]

---
**Bias nei dati**

![[Pasted image 20250311084252.png]]

---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084406.png]]

- **Bias di rappresentazione:** il dataset non rappresenta accuratamente la popolazione.

---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084550.png]]

 - **Bias di campionamento:** alcuni gruppi hanno maggiore probabilitÃ  di essere selezionati.
---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084626.png]]

- **Bias di aggregazione:** trarre conclusioni su individui basandosi su dati aggregati (es. Paradosso di Simpson).

---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084702.png]]

- **Bias di artefatto:** correlazioni spurie tra caratteristiche e etichette.

---
**Regolamentazioni sull'IA**

ğŸ“œ **Regolamento Generale sulla Protezione dei Dati (GDPR)**  

![[Pasted image 20250311155811.png]]

---
**Regolamentazioni sull'IA**

ğŸ“œ **AI Act (Articolo 13, UE)**  
ğŸ” I sistemi di IA ad alto rischio devono essere progettati in modo trasparente e accompagnati da istruzioni chiare.

---
**Regolamentazioni sull'IA**

![[Pasted image 20250311155927.png]]

---
**InterpretabilitÃ  vs. SpiegabilitÃ **

ğŸ“Œ Alcuni modelli sono **inherently interpretable**, (non necessitano di essere spiegati) come:
- Alberi decisionali
- Modelli lineari
- Sistemi basati su regole (*Rule based systems*)
- Programmazione logica induttiva

![[Pasted image 20250624183917.png]]

---
**InterpretabilitÃ  vs. SpiegabilitÃ **

Alcuni dicono che esiste un trade-off fra interpretabilitÃ  e accuratezza... Ã¨ davvero il caso?

![[Pasted image 20250311160053.png]]

- Non sempre! Se un modello interpretabile Ã¨ anche accurato, dovrebbe essere preferito!
- Si usa un modello black-box solo se Ã¨ necessario... oppure se Ã¨ l'unico modello disponibile
- Altrimenti prova a costruire delle spiegazioni post-hoc

---
**Tassonomia degli approcci**

ğŸ“Œ **Interpretabili per progettazione:**
- Alberi decisionali
- Sistemi basati su regole(liste, regole)
- Modelli lineari generalizzati
- Programmazione logica induttiva


---
**Tassonomia degli approcci**

ğŸ“Œ **Spiegazioni post-hoc per modelli black-box:**
- **Spiegazione globale:** costruzione di un modello interpretabile che approssima il sistema.
- **Ispezione del modello:** analisi delle proprietÃ  di un modello opaco, o delle sue predizioni.
- **Spiegazione dellâ€™output:** non ha l'obiettivo di ricostruire il modello opaco del sistema, ma di dare spiegazione di singole predizioni.

---
## â–¶ Bias e correttezza nellâ€™IA

---
**Bias**

"Il bias Ã¨ un errore sistematico nei processi decisionali che porta a risultati ingiusti." (definizione di E.Ferrara, 2023)

Un sistema di machine learning impara e replica patterns di bias presenti nei dati di addestramento.
Identificare e classificare bias Ã¨ importante per garantire decisioni eque e affidabili.

---
**Tipologie di bias nei dati**

- **Bias di campionamento**:
	Si verifica quando i dati di addestramento non rappresentano in modo casuale lâ€™intera popolazione, cioÃ¨ alcuni gruppi hanno una probabilitÃ  piÃ¹ alta di essere selezionati.  (esempio: Il sondaggio _Literary Digest_ per le elezioni presidenziali USA del 1936, basato su un campione non rappresentativo)

- **Bias di rappresentazione**:
	Simile al bias di campionamento, si verifica quando il dataset non riflette accuratamente la popolazione, ad esempio escludendo o sottorappresentando alcuni sottogruppi.  (Esempio: Dataset di riconoscimento immagini con scarsa rappresentazione di culture non occidentali (_es. ImageNet_))

- **Bias di artefatto**:
	Si verifica quando esistono correlazioni spurie tra caratteristiche dei dati e le etichette, portando il modello a prendere decisioni errate.

-  **Bias di aggregazione**  
	Si verifica quando si fanno inferenze sugli individui basandosi su statistiche aggregate dellâ€™intera popolazione.  
	- Esempio: Il **paradosso di Simpson**.

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311174619.png]]

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311174647.png]]

---
**Bias nei dati: Paradosso di Simpson**

Qual Ã¨ il miglior trattamento per una malattia?
- Il **trattamento A** sembra il migliore considerando alcuni gruppi di pazienti.
- Il **trattamento B** sembra il migliore guardando i dati aggregati.

Il paradosso di Simpson si verifica quando una tendenza osservata in piÃ¹ gruppi scompare o si inverte quando i gruppi vengono combinati.

---
**Bias nei dati: Paradosso di Simpson**

Una **Variabile confondente**  Ã¨ una variabile che influenza sia la variabile indipendente che quella dipendente, causando il paradosso.  

![[Pasted image 20250311174836.png]]

Ad esempio: nello studio sulle malattie renali, la **dimensione dei calcoli renali** ha influenzato la scelta del trattamento (i casi piÃ¹ semplici erano trattati con il metodo B).

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311174943.png]]

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311175007.png]]

---
**Bias nei dati: Paradosso di Simpson**

**Principio della cosa sicura(?)**

Un azione A che aumenta la probabilitÃ  di un evento E in ogni sottopopolazione deve anche aumentare la probabilitÃ  di E nell'intera popolazione, in questo modo l'azione non cambia la distribuzione delle sottopopolazioni.

---
**Bias negli algoritmi**

ğŸ“Œ **Bias algoritmico**: 
	Si verifica quando il bias non Ã¨ presente nei dati, ma Ã¨ introdotto dall'algoritmo stesso. 
	**Esempio:** Software antiplagio che penalizza maggiormente le persone non madrelingua inglese perchÃ© compara sequenze di testo piÃ¹ lunghe.

ğŸ“Œ **Bias da interazione**: 
	Si verifica quando l'interazione con gli esseri umani porta il modello a sviluppare comportamenti discriminatori.  
	**Esempio:** Un chatbot che risponde in modo diverso agli uomini e alle donne.

---
**Bias negli algoritmi**

ğŸ“Œ **Bias di conferma**:  
	Accade quando un sistema di IA viene utilizzato per **rafforzare pregiudizi preesistenti** nei dati o nei suoi utenti.  
	**Esempio:** Algoritmi di selezione del personale che premiano candidati con caratteristiche simili ai dipendenti giÃ  assunti.

ğŸ“Œ **Bias generativo**:  
	Si verifica quando un modello generativo produce contenuti che riflettono in modo sproporzionato certi attributi, prospettive o pattern presenti nei dati di addestramento.  
	**Esempio:** Un generatore di immagini che produce sempre figure maschili per il ruolo di "CEO" e femminili per "infermiere".

---
**Concetti di correttezza**

Il bias puÃ² facilmente portare a decisioni ingiuste. Ma **come possiamo definire la correttezza?**  

ğŸ“Œ **Definizione:** _La correttezza Ã¨ lâ€™assenza di pregiudizi o favoritismi nei confronti di un individuo o gruppo, basati su caratteristiche innate o acquisite._

Esistono molte metriche per misurare la **correttezza**. La maggior parte di queste sono basate sull'esistenza di una macchina di machine learning che modella la probabilitÃ  P di una classe positiva e le sue relazioni con qualche attributo protetto A.

---
**Correttezza**

ğŸ“Œ **Equalized Odds** 

Un predittore soddisfa equalized odds con rispetto all'attributo protetto $A$ e outcome $Y$, se $Y$ e $A$ sono indipendenti condizionalmente da $\hat{Y}$
	
	$P( \hat{Y} = 1 | A = 0, Y = \gamma) = P(\hat{Y} = 0 | A = 1, Y = \gamma) \space \gamma \in \{0,1\}$

Un predittore soddisfa questa proprietÃ  se la probabilitÃ  di classificare correttamente un'istanza positiva (TP) e la probabilitÃ  di classificare erroneamente un'istanza negativa (FP) sono le stesse per i gruppi protetti e non protetti.

Gruppi protetti e non dovrebbero avere stessi tassi di  ottenere TP e FP

---
**Correttezza**

ğŸ“Œ **Equal Opportunity** 

Un predittore binario $Y$ soddisfa Equal opportunity con rispetto a $A$ e $Y$ se:

	$P( \hat{Y} = 1 | A = 0, Y = 1) = P(\hat{Y} = 0 | A = 1, Y = 1)$

Un predittore soddisfa questa proprietÃ  se la probabilitÃ  di classificare correttamente un'istanza positiva Ã¨ la stessa per i gruppi protetti e non protetti.

Gruppi protetti e non dovrebbero avere stessi tassi di TP

---
**Correttezza**

ğŸ“Œ **ParitÃ  Demografica (Demographic Parity)**  

Un predittore $Y$ soddisfa demographic parity se:

	$P( \hat{Y} | A = 0) = P(\hat{Y}| A = 1)$
	
Un predittore soddisfa questa proprietÃ  se la probabilitÃ  (in questo caso likelihood) di ottenere un esito positivo Ã¨ la stessa indipendentemente dall'appartenenza a un gruppo protetto.

---
**Metodi per garantire la correttezza**

ğŸ“Œ **Fairness through awareness**  
	Un algoritmo Ã¨ equo se assegna risultati simili a individui con caratteristiche simili, secondo una metrica di somiglianza definita per il compito specifico.

ğŸ“Œ **Fairness through unawareness**  
	Un algoritmo Ã¨ equo se **non utilizza** esplicitamente caratteristiche protette nella presa di decisione.

---
**Metodi per garantire la correttezza**

ğŸ“Œ **Altri criteri di correttezza**
- **Uguaglianza di trattamento**
- **EquitÃ  nei test**
- **Correttezza controfattuale**
- **EquitÃ  nei domini relazionali**
- **ParitÃ  statistica condizionale**

---
**Come applicare la correttezza nei modelli?**

Come gestire il bias?

1. **Pre-processing:** Modificare i dati prima dellâ€™addestramento per eliminare discriminazioni. 
2. **In-processing:** Modificare gli algoritmi di apprendimento per garantire equitÃ  (ad es. cambiando la funzione obiettivo o introducendo vincoli).  
3. **Post-processing:** Intervenire dopo lâ€™addestramento, ad esempio correggendo le predizioni tramite set di validazione.

---
## â–¶ Valutazione

---
**Valutazione della spiegabilitÃ **

**Valutare la spiegabilitÃ  e l'interpretabilitÃ  non Ã¨ semplice!**  
Ci sono diversi approcci per affrontare il problema:

- Il sistema funziona come previsto?
- Gli utenti del sistema vengono trattati equamente?
- Il sistema Ã¨ conforme alla normativa?
- Le spiegazioni devono essere valutate nel contesto dellâ€™applicazione?
- Possiamo usare misure quantitative per valutare le spiegazioni?
- Alcune spiegazioni sono migliori di altre?

---
**Valutazione della spiegabilitÃ **

![[Pasted image 20250312185203.png]]

---

**Tipologie di valutazione della spiegabilitÃ **

ğŸ“Œ **Valutazione basata su applicazioni reali (Application-grounded evaluation)**  

- Persone reali, task reali  
- Esperimenti condotti con esperti del dominio su un **compito reale**.  
- Metodo piÃ¹ affidabile, ma anche il piÃ¹ costoso.

---
**Tipologie di valutazione della spiegabilitÃ **

ğŸ“Œ **Valutazione con utenti umani generici (Human-grounded evaluation)**  

- Esperimenti con **non esperti**, su compiti semplificati vicini allâ€™applicazione reale.  
- Permette di testare un **numero maggiore di utenti**, ma Ã¨ meno preciso (anche se meno costoso)

---
**Tipologie di valutazione della spiegabilitÃ **

ğŸ“Œ **Valutazione basata su metriche computazionali (Functionally-grounded evaluation)**  

- **Non richiede esperimenti con esseri umani**, utile quando non etico o replicabile.  
- Usa metriche **computazionali di proxy**, come lâ€™importanza delle caratteristiche.  
- Utile quando il modello Ã¨ giÃ  stato **validato da esperti umani**. Oppure quando un metodo non Ã¨ ancora maturo

---
**Valutazione della spiegabilitÃ **

ğŸ“Œ **Metriche di valutazione quantitativa** (Metriche con contributo umano):
- **Completezza:** La spiegazione Ã¨ completa per lâ€™utente?
- **SemplicitÃ :** Preferenza per spiegazioni piÃ¹ semplici (_Rasoio di Occam_).
- **ComprensibilitÃ :** Quanto tempo impiega un essere umano a capire la spiegazione?
- **PlausibilitÃ :** Quanto la spiegazione Ã¨ convincente per lâ€™utente?
- **SimulabilitÃ :** Lâ€™utente puÃ² applicare la spiegazione su nuovi dati?
- **Rilevanza:** Metodologie specifiche per diversi settori (clinico, giuridico, ecc.).

---
**Valutazione della spiegabilitÃ **

 ğŸ“Œ **Metriche ausiliarie (proxy):**
- **SensibilitÃ :** Quanto un modello Ã¨ influenzato da un determinato attributo?
- **ContinuitÃ :** Esempi simili dovrebbero avere spiegazioni simili.
- **Consistenza:** La spiegazione Ã¨ coerente tra diversi modelli?
- **Costo computazionale:** Quanto tempo richiede il calcolo della spiegazione?
- **Correttezza:** Confronto con un _ground-truth_ noto.

---

## â–¶ Analisi esplorativa dei dati

---
ğŸ“Œ **Principali tecniche di visualizzazione dati:**

- **Analisi univariata:** statistiche descrittive, istogrammi, box plot, ecc.
- **Analisi multivariata:** correlazioni, interazioni tra variabili, ecc.
- **Distribuzione dei dati:** verifica delle caratteristiche di ciascuna feature.
- **Identificazione di outlier:** rilevamento di dati anomali o rumorosi.

---
**Visualizzazione dati**

Attenzione alle statistiche descrittive!

ğŸ“Œ **Esempio: Il quartetto di Anscombe**  

![[Pasted image 20250313144804.png]]

Il quartetto di Anscombe Ã¨ un insieme di **quattro dataset** creati in modo che abbiano le **stesse statistiche riassuntive (media, varianza, correlazione), ma distribuzioni molto diverse**.

![[Pasted image 20250313144856.png]]

âš ï¸ Questo dimostra che **guardare solo le statistiche numeriche puÃ² essere fuorviante**: Ã¨ sempre necessaria un'analisi visiva dei dati.

![[Pasted image 20250313144924.png]]

---
**Visualizzazione dati**

ğŸ“Œ **Strumenti per la visualizzazione dei dati**

![[Pasted image 20250313145033.png]]

ğŸ“Œ **Librerie Python piÃ¹ utilizzate:**  
- **Classiche:** `pandas`, `numpy`, `matplotlib`  
- **PiÃ¹ recenti:** `seaborn`, `plotly`, `vega-altair`  
- **Specializzate:** `ydata-profiling`, `FACETS`, `KNIME`  
- **Strumenti interattivi** per lâ€™analisi dinamica dei dati

ğŸ”— **Riferimento:** [Distill - Interactive Data Communication](https://distill.pub/2020/communicating-with-interactive-articles/)

---

## â–¶ Esempi

---
ğŸ“Œ **Importanza delle caratteristiche (Feature Importance)**  

![[Pasted image 20250313145134.png]]

âœ”ï¸ Indica quanto ogni variabile contribuisce alla predizione del modello.

--- 
ğŸ“Œ **Mappe di attenzione (Attention Maps)**  

![[Pasted image 20250313145214.png]]

âœ”ï¸ Visualizzano quali parti dellâ€™input il modello considera piÃ¹ rilevanti.

---

ğŸ“Œ **Mappe di salienza (Saliency Maps)**  

![[Pasted image 20250313145243.png]]

âœ”ï¸ Evidenziano le aree piÃ¹ influenti dellâ€™immagine per una predizione.

---
ğŸ“Œ **LIME (Local Interpretable Model-agnostic Explanations)**  

![[Pasted image 20250313145315.png]]

âœ”ï¸ Genera spiegazioni interpretabili per singole predizioni dei modelli _black-box_.

---
ğŸ“Œ **SHAP (Shapley Additive Explanations)**  

![[Pasted image 20250313145349.png]]

âœ”ï¸ Metodo basato sulla teoria dei giochi per assegnare **valori di contributo equi** a ciascuna caratteristica del modello.

---

