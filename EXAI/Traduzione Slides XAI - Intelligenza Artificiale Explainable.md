
---
# **00 ‚Äì Informazioni Amministrative**  

MSc in Intelligenza Artificiale  
MSc in Ingegneria Informatica  
**Marco Lippi**  
üìß [marco.lippi@unifi.it](mailto:marco.lippi@unifi.it)

---
**Indice dei contenuti**

## ‚ñ∂ Organizzazione del corso

---
**Contenuto del corso (1/5)**

Introduzione e concetti generali
- Intelligenza artificiale sicura e affidabile: motivazioni e sfide
- Intelligenza artificiale spiegabile: tassonomia, metriche
- Interpretabilit√† progettuale
- Spiegabilit√† dei modelli black-box
- Applicazioni: giustizia, medicina, sistemi critici per la sicurezza

---
**Contenuto del corso (2/5)**

Modelli interpretabili
- Approcci iniziali: alberi decisionali, modelli lineari generalizzati, liste di regole
- Explainable Boosting Machines
- Modelli basati su prototipi, incorporamento di concetti
- Alberi decisionali ottimali
- Programmazione logica induttiva
- Argomentazione

---
**Contenuto del corso (3/5)**

Spiegazioni post-hoc
- SHAP
- LIME
- Attenzione, mappe di salienza
- Analisi di sensibilit√†

---
**Contenuto del corso (4/5)**

Intelligenza artificiale neuro-simbolica
- Combinazione di IA simbolica e sottosimbolica
- Logica di Markov, DeepProbLog
- Modelli a colli di bottiglia concettuali

---
**Contenuto del corso (5/5)**

Applicazioni e sessioni pratiche
- Librerie Python per metodi specifici
- Manipolazione e visualizzazione dei dati
- Ispezione dei modelli
- Presentazione dei risultati, calcolo delle metriche
- Problemi reali e casi studio

---
**Orario delle lezioni**

üìå **Lezioni:**
- Marted√¨, 14:00-16:00, aula S12 @ Santa Marta (14:00-15:30)
- Gioved√¨, 16:00-18:00, aula 177 @ Santa Marta (16:00-17:30)

üìå **Orario di ricevimento:**
- Marted√¨, 11:30-13:30 (ufficio @ DINFO, Santa Marta)
- Oppure su appuntamento via email
- In ogni caso, scrivetemi sempre in anticipo!

---
**Materiale didattico**

Il materiale didattico sar√† disponibile su **Moodle**:
- Slide (a volte contenenti solo riassunti degli argomenti)
- Appunti dalle lavagne
- Riferimenti a articoli scientifici e capitoli di libri

üì¢ **Iscrivetevi alla pagina Moodle per:**
- Informazioni sul corso
- Variazioni di orario, annunci
- Aggiornamenti del materiale didattico
- Informazioni sull‚Äôorganizzazione dell‚Äôesame

---
**Esame**

L‚Äôesame consiste in una prova orale sui contenuti del corso:
- **Iscrizione** tramite SOL
- **Date disponibili:**
    - Estate: 17 giugno, 1¬∞ luglio, 21 luglio
    - Autunno: 1¬∞ settembre
    - Inverno/primavera: date da definire

---
**Progetti di laboratorio**

Gli studenti che hanno scelto il **Laboratorio (3 CFU)** nel loro piano di studi svolgeranno un progetto sull‚Äôapplicazione delle tecniche di **XAI** a problemi reali:

- Diritto
- Medicina
- Sistemi critici per la sicurezza
- Astronomia
- Bibliometria
- ‚Ä¶

---

# **01 ‚Äì Introduzione**  

---
**Indice dei contenuti**

‚ñ∂ Concetti generali  
‚ñ∂ Bias e correttezza  
‚ñ∂ Valutazione  
‚ñ∂ Analisi esplorativa dei dati  
‚ñ∂ Esempi

---

## ‚ñ∂ Concetti generali  

---
**Crediti**

Queste slide sono in gran parte un adattamento di materiale esistente, tra cui:

- _Explainable Artificial Intelligence_ (F. Lecue et al., 2023)
- _Explainable and Trustworthy Artificial Intelligence_ (Politecnico di Torino, 2023)
- _Explainable AI_ (Harvard University, 2023)

üîó **Riferimenti:**

- [XAI Tutorial 2023](https://xaitutorial2023.github.io/)
- [Politecnico di Torino](https://dbdmg.polito.it/dbdmg_web/2024/explainable-and-trustworthy-ai-2023-2024/)
- [Interpretable ML Class](https://interpretable-ml-class.github.io/)

---
**Cos'√® XAI?**

![[Pasted image 20250311082740.png]]

L'Intelligenza Artificiale Spiegabile (_Explainable AI_, XAI) studia e sviluppa **metodi** per rendere **accessibile** e **interpretabile** la logica interna e i risultati dei modelli di intelligenza artificiale, rendendoli **comprensibili agli esseri umani**.

---
**Modelli "scatola nera"**

![[Pasted image 20250311082850.png]]

---
**Modelli "scatola nera"**

![[Pasted image 20250311082923.png]]

---
**Perch√© abbiamo bisogno della XAI?**

1Ô∏è‚É£ **Per comprendere le decisioni dell'IA**  

![[Pasted image 20250311083033.png]]

---
**Perch√© abbiamo bisogno della XAI?**

2Ô∏è‚É£ **Per migliorare i modelli di IA**  

![[Pasted image 20250311083124.png]]

---
**Perch√© abbiamo bisogno della XAI?**

3Ô∏è‚É£ **Per costruire sistemi di IA affidabili e sicuri**

![[Pasted image 20250311083138.png]]

---
**Il problema dell'allineamento**

üìñ _"Se utilizziamo un meccanismo che non possiamo controllare in modo efficace‚Ä¶ dobbiamo essere certi che lo scopo inserito nella macchina sia quello che desideriamo davvero."_                                         ‚Äî Norbert Wiener, 1960

---
**Il problema dell'allineamento**

ü§ñ _Le Tre Leggi della Robotica_ (Isaac Asimov, 1942):  
	1.  Un robot non pu√≤ arrecare danno a un essere umano, n√© permettere che un essere umano subisca danni per la sua inazione.  
	2. Un robot deve obbedire agli ordini impartiti dagli esseri umani, a meno che ci√≤ non contrasti con la Prima Legge. 
	3. Un robot deve proteggere la propria esistenza, purch√© questa protezione non contrasti con la Prima o la Seconda Legge.

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

![[Pasted image 20250311083519.png]]

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

![[Pasted image 20250311083536.png]]

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

![[Pasted image 20250311083915.png]]

---
**Rischi e pericoli dell'uso (o abuso) dell'IA**

**Esempio: Predizione del rischio di polmonite con IA**
Un modello di IA potrebbe imparare erroneamente che i pazienti asmatici hanno un rischio minore di complicazioni perch√© sono ricoverati **direttamente in terapia intensiva**. Questo porta a conclusioni fuorvianti se si considerano solo i dati storici!

---
**Modelli black-box possono essere presi in giro**

![[Pasted image 20250311084154.png]]

---
**Bias nei dati**

üìå "Perch√© dovrei fidarmi del tuo modello?" (Ribeiro et al.)  

![[Pasted image 20250311084216.png]]

---
**Bias nei dati**

![[Pasted image 20250311084252.png]]

---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084406.png]]

- **Bias di rappresentazione:** il dataset non rappresenta accuratamente la popolazione.

---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084550.png]]

 - **Bias di campionamento:** alcuni gruppi hanno maggiore probabilit√† di essere selezionati.
---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084626.png]]

- **Bias di aggregazione:** trarre conclusioni su individui basandosi su dati aggregati (es. Paradosso di Simpson).

---
**Tipologie di Bias nei dati**

![[Pasted image 20250311084702.png]]

- **Bias di artefatto:** correlazioni spurie tra caratteristiche e etichette.

---
**Regolamentazioni sull'IA**

üìú **Regolamento Generale sulla Protezione dei Dati (GDPR)**  

![[Pasted image 20250311155811.png]]

---
**Regolamentazioni sull'IA**

üìú **AI Act (Articolo 13, UE)**  
üîé I sistemi di IA ad alto rischio devono essere progettati in modo trasparente e accompagnati da istruzioni chiare.

---
**Regolamentazioni sull'IA**

![[Pasted image 20250311155927.png]]

---
**Interpretabilit√† vs. Spiegabilit√†**

üìå Alcuni modelli sono **inherently interpretable**, (non necessitano di essere spiegati) come:
- Alberi decisionali
- Modelli lineari
- Sistemi basati su regole
- Programmazione logica induttiva

---
**Interpretabilit√† vs. Spiegabilit√†**

Alcuni dicono che esiste un trade-off fra interpretabilit√† e accuratezza... √® davvero il caso?

![[Pasted image 20250311160053.png]]

- Non sempre! Se un modello interpretabile √® anche accurato, dovrebbe essere preferito!
- Si usa un modello black-box solo se √® necessario... oppure se √® l'unico modello disponibile
- Altrimenti prova a costruire delle spiegazioni post-hoc

---
**Tassonomia degli approcci**

üìå **Interpretabili per progettazione:**
- Alberi decisionali
- Sistemi basati su regole(liste, regole)
- Modelli lineari generalizzati
- Programmazione logica induttiva

---

üìå **Spiegazioni post-hoc per modelli black-box:**
- **Spiegazione globale:** costruzione di un modello interpretabile che approssima il sistema.
- **Ispezione del modello:** analisi delle propriet√† di un modello opaco, o delle sue predizioni.
- **Spiegazione dell‚Äôoutput:** non ha l'obiettivo di ricostruire il modello opaco del sistema, ma di dare spiegazione di singole predizioni.

---
## ‚ñ∂ Bias e correttezza nell‚ÄôIA

---
**Bias**

"Il bias √® un errore sistematico nei processi decisionali che porta a risultati ingiusti." (definizione di E.Ferrara, 2023)

Un sistema di machine learning impara e replica patterns di bias presenti nei dati di addestramento.
Identificare e classificare bias √® importante per garantire decisioni eque e affidabili.

---
**Tipologie di bias nei dati**

- **Bias di campionamento**:
	Si verifica quando i dati di addestramento non rappresentano in modo casuale l‚Äôintera popolazione, cio√® alcuni gruppi hanno una probabilit√† pi√π alta di essere selezionati.  (esempio: Il sondaggio _Literary Digest_ per le elezioni presidenziali USA del 1936, basato su un campione non rappresentativo)

- **Bias di rappresentazione**:
	Simile al bias di campionamento, si verifica quando il dataset non riflette accuratamente la popolazione, ad esempio escludendo o sottorappresentando alcuni sottogruppi.  (Esempio: Dataset di riconoscimento immagini con scarsa rappresentazione di culture non occidentali (_es. ImageNet_))

- **Bias di artefatto**:
	Si verifica quando esistono correlazioni spurie tra caratteristiche dei dati e le etichette, portando il modello a prendere decisioni errate.

-  **Bias di aggregazione**  
	Si verifica quando si fanno inferenze sugli individui basandosi su statistiche aggregate dell‚Äôintera popolazione.  
	- Esempio: Il **paradosso di Simpson**.

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311174619.png]]

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311174647.png]]

---
**Bias nei dati: Paradosso di Simpson**

Qual √® il miglior trattamento per una malattia?
- Il **trattamento A** sembra il migliore considerando alcuni gruppi di pazienti.
- Il **trattamento B** sembra il migliore guardando i dati aggregati.

Il paradosso di Simpson si verifica quando una tendenza osservata in pi√π gruppi scompare o si inverte quando i gruppi vengono combinati.

---
**Bias nei dati: Paradosso di Simpson**

Una **Variabile confondente**  √® una variabile che influenza sia la variabile indipendente che quella dipendente, causando il paradosso.  

![[Pasted image 20250311174836.png]]

Ad esempio: nello studio sulle malattie renali, la **dimensione dei calcoli renali** ha influenzato la scelta del trattamento (i casi pi√π semplici erano trattati con il metodo B).

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311174943.png]]

---
**Bias nei dati: Paradosso di Simpson**

![[Pasted image 20250311175007.png]]

---
**Bias nei dati: Paradosso di Simpson**

**Principio della cosa sicura(?)**

Un azione A che aumenta la probabilit√† di un evento E in ogni sottopopolazione deve anche aumentare la probabilit√† di E nell'intera popolazione, in questo modo l'azione non cambia la distribuzione delle sottopopolazioni.

---
**Bias negli algoritmi**

üìå **Bias algoritmico**: 
	Si verifica quando il bias non √® presente nei dati, ma √® introdotto dall'algoritmo stesso. 
	**Esempio:** Software antiplagio che penalizza maggiormente le persone non madrelingua inglese perch√© compara sequenze di testo pi√π lunghe.

üìå **Bias da interazione**: 
	Si verifica quando l'interazione con gli esseri umani porta il modello a sviluppare comportamenti discriminatori.  
	**Esempio:** Un chatbot che risponde in modo diverso agli uomini e alle donne.

---
**Bias negli algoritmi**

üìå **Bias di conferma**:  
	Accade quando un sistema di IA viene utilizzato per **rafforzare pregiudizi preesistenti** nei dati o nei suoi utenti.  
	**Esempio:** Algoritmi di selezione del personale che premiano candidati con caratteristiche simili ai dipendenti gi√† assunti.

üìå **Bias generativo**:  
	Si verifica quando un modello generativo produce contenuti che riflettono in modo sproporzionato certi attributi, prospettive o pattern presenti nei dati di addestramento.  
	**Esempio:** Un generatore di immagini che produce sempre figure maschili per il ruolo di "CEO" e femminili per "infermiere".

---
**Concetti di correttezza**

Il bias pu√≤ facilmente portare a decisioni ingiuste. Ma **come possiamo definire la correttezza?**  

üìå **Definizione:** _La correttezza √® l‚Äôassenza di pregiudizi o favoritismi nei confronti di un individuo o gruppo, basati su caratteristiche innate o acquisite._

Esistono molte metriche per misurare la **correttezza**. La maggior parte di queste sono basate sull'esistenza di una macchina di machine learning che modella la probabilit√† P di una classe positiva e le sue relazioni con qualche attributo protetto A.

---
**Correttezza**

üìå **Equalized Odds** 

Un predittore soddisfa equalized odds con rispetto all'attributo protetto $A$ e outcome $Y$, se $Y$ e $A$ sono indipendenti condizionalmente da $\hat{Y}$
	
	$P( \hat{Y} = 1 | A = 0, Y = \gamma) = P(\hat{Y} = 0 | A = 1, Y = \gamma) \space \gamma \in \{0,1\}$

Un predittore soddisfa questa propriet√† se la probabilit√† di classificare correttamente un'istanza positiva (TP) e la probabilit√† di classificare erroneamente un'istanza negativa (FP) sono le stesse per i gruppi protetti e non protetti.

Gruppi protetti e non dovrebbero avere stessi tassi di  ottenere TP e FP

---
**Correttezza**

üìå **Equal Opportunity** 

Un predittore binario $Y$ soddisfa Equal opportunity con rispetto a $A$ e $Y$ se:

	$P( \hat{Y} = 1 | A = 0, Y = 1) = P(\hat{Y} = 0 | A = 1, Y = 1)$

Un predittore soddisfa questa propriet√† se la probabilit√† di classificare correttamente un'istanza positiva √® la stessa per i gruppi protetti e non protetti.

Gruppi protetti e non dovrebbero avere stessi tassi di TP

---
**Correttezza**

üìå **Parit√† Demografica (Demographic Parity)**  

Un predittore $Y$ soddisfa demographic parity se:

	$P( \hat{Y} | A = 0) = P(\hat{Y}| A = 1)$
	
Un predittore soddisfa questa propriet√† se la probabilit√† (in questo caso likelihood) di ottenere un esito positivo √® la stessa indipendentemente dall'appartenenza a un gruppo protetto.

---
**Metodi per garantire la correttezza**

üìå **Fairness through awareness**  
	Un algoritmo √® equo se assegna risultati simili a individui con caratteristiche simili, secondo una metrica di somiglianza definita per il compito specifico.

üìå **Fairness through unawareness**  
	Un algoritmo √® equo se **non utilizza** esplicitamente caratteristiche protette nella presa di decisione.

---
**Metodi per garantire la correttezza**

üìå **Altri criteri di correttezza**
- **Uguaglianza di trattamento**
- **Equit√† nei test**
- **Correttezza controfattuale**
- **Equit√† nei domini relazionali**
- **Parit√† statistica condizionale**

---
**Come applicare la correttezza nei modelli?**

Come gestire il bias?

1. **Pre-processing:** Modificare i dati prima dell‚Äôaddestramento per eliminare discriminazioni. 
2. **In-processing:** Modificare gli algoritmi di apprendimento per garantire equit√† (ad es. cambiando la funzione obiettivo o introducendo vincoli).  
3. **Post-processing:** Intervenire dopo l‚Äôaddestramento, ad esempio correggendo le predizioni tramite set di validazione.

---
## ‚ñ∂ Valutazione

---
**Valutazione della spiegabilit√†**

**Valutare la spiegabilit√† e l'interpretabilit√† non √® semplice!**  
Ci sono diversi approcci per affrontare il problema:

- Il sistema funziona come previsto?
- Gli utenti del sistema vengono trattati equamente?
- Il sistema √® conforme alla normativa?
- Le spiegazioni devono essere valutate nel contesto dell‚Äôapplicazione?
- Possiamo usare misure quantitative per valutare le spiegazioni?
- Alcune spiegazioni sono migliori di altre?

---
**Valutazione della spiegabilit√†**

![[Pasted image 20250312185203.png]]

---

**Tipologie di valutazione della spiegabilit√†**

üìå **Valutazione basata su applicazioni reali (Application-grounded evaluation)**  

- Persone reali, task reali  
- Esperimenti condotti con esperti del dominio su un **compito reale**.  
- Metodo pi√π affidabile, ma anche il pi√π costoso.

---
**Tipologie di valutazione della spiegabilit√†**

üìå **Valutazione con utenti umani generici (Human-grounded evaluation)**  

- Esperimenti con **non esperti**, su compiti semplificati vicini all‚Äôapplicazione reale.  
- Permette di testare un **numero maggiore di utenti**, ma √® meno preciso (anche se meno costoso)

---
**Tipologie di valutazione della spiegabilit√†**

üìå **Valutazione basata su metriche computazionali (Functionally-grounded evaluation)**  

- **Non richiede esperimenti con esseri umani**, utile quando non etico o replicabile.  
- Usa metriche **computazionali di proxy**, come l‚Äôimportanza delle caratteristiche.  
- Utile quando il modello √® gi√† stato **validato da esperti umani**. Oppure quando un metodo non √® ancora maturo

---
**Valutazione della spiegabilit√†**

üìå **Metriche di valutazione quantitativa** (Metriche con contributo umano):
- **Completezza:** La spiegazione √® completa per l‚Äôutente?
- **Semplicit√†:** Preferenza per spiegazioni pi√π semplici (_Rasoio di Occam_).
- **Comprensibilit√†:** Quanto tempo impiega un essere umano a capire la spiegazione?
- **Plausibilit√†:** Quanto la spiegazione √® convincente per l‚Äôutente?
- **Simulabilit√†:** L‚Äôutente pu√≤ applicare la spiegazione su nuovi dati?
- **Rilevanza:** Metodologie specifiche per diversi settori (clinico, giuridico, ecc.).

---
**Valutazione della spiegabilit√†**

 üìå **Metriche ausiliarie (proxy):**
- **Sensibilit√†:** Quanto un modello √® influenzato da un determinato attributo?
- **Continuit√†:** Esempi simili dovrebbero avere spiegazioni simili.
- **Consistenza:** La spiegazione √® coerente tra diversi modelli?
- **Costo computazionale:** Quanto tempo richiede il calcolo della spiegazione?
- **Correttezza:** Confronto con un _ground-truth_ noto.

---

## ‚ñ∂ Analisi esplorativa dei dati

---
üìå **Principali tecniche di visualizzazione dati:**

- **Analisi univariata:** statistiche descrittive, istogrammi, box plot, ecc.
- **Analisi multivariata:** correlazioni, interazioni tra variabili, ecc.
- **Distribuzione dei dati:** verifica delle caratteristiche di ciascuna feature.
- **Identificazione di outlier:** rilevamento di dati anomali o rumorosi.

---
**Visualizzazione dati**

Attenzione alle statistiche descrittive!

üìå **Esempio: Il quartetto di Anscombe**  

![[Pasted image 20250313144804.png]]

Il quartetto di Anscombe √® un insieme di **quattro dataset** creati in modo che abbiano le **stesse statistiche riassuntive (media, varianza, correlazione), ma distribuzioni molto diverse**.

![[Pasted image 20250313144856.png]]

‚ö†Ô∏è Questo dimostra che **guardare solo le statistiche numeriche pu√≤ essere fuorviante**: √® sempre necessaria un'analisi visiva dei dati.

![[Pasted image 20250313144924.png]]

---
**Visualizzazione dati**

üìå **Strumenti per la visualizzazione dei dati**

![[Pasted image 20250313145033.png]]

üìå **Librerie Python pi√π utilizzate:**  
- **Classiche:** `pandas`, `numpy`, `matplotlib`  
- **Pi√π recenti:** `seaborn`, `plotly`, `vega-altair`  
- **Specializzate:** `ydata-profiling`, `FACETS`, `KNIME`  
- **Strumenti interattivi** per l‚Äôanalisi dinamica dei dati

üîó **Riferimento:** [Distill - Interactive Data Communication](https://distill.pub/2020/communicating-with-interactive-articles/)

---

## ‚ñ∂ Esempi

---
üìå **Importanza delle caratteristiche (Feature Importance)**  

![[Pasted image 20250313145134.png]]

‚úîÔ∏è Indica quanto ogni variabile contribuisce alla predizione del modello.

--- 
üìå **Mappe di attenzione (Attention Maps)**  

![[Pasted image 20250313145214.png]]

‚úîÔ∏è Visualizzano quali parti dell‚Äôinput il modello considera pi√π rilevanti.

---

üìå **Mappe di salienza (Saliency Maps)**  

![[Pasted image 20250313145243.png]]

‚úîÔ∏è Evidenziano le aree pi√π influenti dell‚Äôimmagine per una predizione.

---
üìå **LIME (Local Interpretable Model-agnostic Explanations)**  

![[Pasted image 20250313145315.png]]

‚úîÔ∏è Genera spiegazioni interpretabili per singole predizioni dei modelli _black-box_.

---
üìå **SHAP (Shapley Additive Explanations)**  

![[Pasted image 20250313145349.png]]

‚úîÔ∏è Metodo basato sulla teoria dei giochi per assegnare **valori di contributo equi** a ciascuna caratteristica del modello.

---

